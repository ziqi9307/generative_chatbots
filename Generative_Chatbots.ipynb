{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generative_Chatbots.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNzK4dZfHmUwX7yY3+aDPmE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziqi9307/generative_chatbots/blob/main/Generative_Chatbots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative Chatbots Using Reccurent Neural Nets"
      ],
      "metadata": {
        "id": "NhF5TCuaqFa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_fPtmCgNqLqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Required Files"
      ],
      "metadata": {
        "id": "a0GcvFfBqNXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://content.codecademy.com/programs/chatbots/generative-chatbots/twitter-project.zip"
      ],
      "metadata": {
        "id": "sfX74aeQqQ5M",
        "outputId": "913bc042-aa38-4de2-bc68-e1f0301e488f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 11552  100 11552    0     0  62782      0 --:--:-- --:--:-- --:--:-- 62782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hTG6TCxnsKyI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "6z6EL2UhpiH9",
        "outputId": "87d87275-f78d-49b6-aba9-3e417316b9e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  twitter-project.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_kiPpqBGqu8G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq twitter-project.zip"
      ],
      "metadata": {
        "id": "cWp0L7L5plVh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f twitter-project.zip"
      ],
      "metadata": {
        "id": "H0IXxdl9wJTe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "XoLwCLHzq2kP",
        "outputId": "98644f07-adcf-44e1-bf93-323505b00d54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__MACOSX  twitter-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r __MACOSX/"
      ],
      "metadata": {
        "id": "I2gdMcdCq3mC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r sample_data"
      ],
      "metadata": {
        "id": "bHHFotxnwVTf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv twitter-project/* ."
      ],
      "metadata": {
        "id": "5wKLdDWAweVM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r twitter-project/"
      ],
      "metadata": {
        "id": "wqvOn7GKwjRn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DLeY44DFwTDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "061elnAArNA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LvuoG1i8rTPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**<br>\n",
        "Open twitter_prep.py and preprocessing.py in a code editor or IDE.\n",
        "In twitter_prep.py, change data_path to the file path of [your-topic].txt. If it’s in the same directory as twitter_prep.py, then all you need is the file name. <br>\n",
        "\n",
        "At the bottom of twitter_prep.py, print out all the response pairs from the data. Run the file to see the format they are in. Then comment the print statement out. <br>\n",
        "\n",
        "In preprocessing.py, adjust the number of lines of training data you want to work with. We’re giving you a default of 15, but depending on how much you want to tax your computer, you can go up to -1 (the last line of the file).\n",
        "Run preprocessing.py and make sure everything works, error-free."
      ],
      "metadata": {
        "id": "on2a34bArhaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import zip_longest\n",
        "import re\n",
        "\n",
        "\n",
        "data_path = \"twitter-project/cat.txt\"\n",
        "\n",
        "# Defining lines as a list of each line\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "  lines = f.read().split('\\n')\n",
        "\n",
        "\n",
        "lines = [re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", line).strip() for line in lines]\n",
        "\n",
        "# group lines by response pair\n",
        "def grouper(iterable, n, fillvalue=None):\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip_longest(*args, fillvalue=fillvalue)\n",
        "\n",
        "pairs = list(grouper(lines, 2))\n",
        "len(pairs)"
      ],
      "metadata": {
        "id": "eVBYTL1Irpcr",
        "outputId": "3be290ac-049a-4631-a84f-59cd7c958ac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Js2ym5YQsWax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Pqh4EXLZscA2",
        "outputId": "ac99e795-34e1-437a-bcbf-6fa881b3195d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "twitter-project/cat.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "K2NlojSVtgXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open training_model.py.\n",
        "Change the values for the following: <br><br>\n",
        "**batch_size:** You can choose to adjust this or not at this point. This determines how many response pairs are used at a time for training. <br>\n",
        "**epochs:** This should be a larger number so that the seq2seq model has many chances to improve. Bear in mind that a larger number of epochs will also take your computer a lot longer to process. If you don’t have the ability to leave your computer running awhile for this project, then choose a number that is less than 500. <br><br>\n",
        "<hr><br>\n",
        "\n",
        "Run the code to generate your model. In the terminal, you should see a summary of the model printed out. Meanwhile, you’ll also see a new file in the directory called training_model.h5. This is where your seq2seq training model is saved so that it’s quicker for you to run your code when you instantiate the chatbot. <br>\n",
        "Note that you may get the following error when attempting to run your program on a regular computer that uses CPU processing:\n",
        "\n",
        "OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.\n",
        "OMP: Hint: This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\n",
        "Abort trap: 6"
      ],
      "metadata": {
        "id": "riUZFlrXtlBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing import num_encoder_tokens, num_decoder_tokens, decoder_target_data, encoder_input_data, decoder_input_data, decoder_target_data, max_encoder_seq_length, max_decoder_seq_length\n",
        "\n",
        "from tensorflow import keras\n",
        "# Add Dense to the imported layers\n",
        "from keras.layers import Input, LSTM, Dense, Masking\n",
        "from keras.models import Model\n",
        "import os\n",
        "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "# Choose dimensionality\n",
        "dimensionality = 256\n",
        "\n",
        "# Choose the batch size\n",
        "# and number of epochs:\n",
        "batch_size = 10\n",
        "epochs = 50\n",
        "\n",
        "# Encoder training setup\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "\n",
        "encoder_lstm = LSTM(dimensionality, return_state=True)\n",
        "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [state_hidden, state_cell]\n",
        "\n",
        "# Decoder training setup:\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(dimensionality, return_sequences=True, return_state=True)\n",
        "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Building the training model:\n",
        "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model:\n",
        "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')\n",
        "\n",
        "# print(\"Training the model:\\n\")\n",
        "# Train the model:\n",
        "training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n",
        "\n",
        "training_model.save('training_model.h5')"
      ],
      "metadata": {
        "id": "8vLgl91JsfAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}